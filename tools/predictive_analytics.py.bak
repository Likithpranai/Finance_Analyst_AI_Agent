"""
Predictive Analytics Tools for Finance Analyst AI Agent

This module provides advanced predictive analytics capabilities including:
- Time series forecasting with Prophet (with ARIMA fallback)
- LSTM/GRU-based predictions (when TensorFlow is available)
- Volatility modeling with regime detection
- Advanced anomaly detection with multiple algorithms
- Scenario analysis with Monte Carlo simulations
- Stationarity testing and time series decomposition
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Tuple
import warnings
import base64
from io import BytesIO
import json

# Data preprocessing
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.decomposition import PCA

# Anomaly detection models
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
from sklearn.neighbors import LocalOutlierFactor

# Time series analysis
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# ARIMA models as fallback for Prophet
try:
    from statsmodels.tsa.arima.model import ARIMA
    from statsmodels.tsa.statespace.sarimax import SARIMAX
    ARIMA_AVAILABLE = True
except ImportError:
    ARIMA_AVAILABLE = False
    warnings.warn("ARIMA models not available. Using simpler fallbacks.")

# Import Prophet for time series forecasting
try:
    from prophet import Prophet
    from prophet.diagnostics import cross_validation, performance_metrics
    PROPHET_AVAILABLE = True
except ImportError:
    PROPHET_AVAILABLE = False
    warnings.warn("Prophet not available. Using ARIMA as fallback.")

# Import Plotly for interactive visualizations
try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False
    warnings.warn("Plotly not available. Using Matplotlib for visualizations.")

# Import scikit-learn components for nearest neighbors
try:
    from sklearn.neighbors import NearestNeighbors
    SKLEARN_NEIGHBORS_AVAILABLE = True
except ImportError:
    SKLEARN_NEIGHBORS_AVAILABLE = False

# Try importing TensorFlow for deep learning models
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, load_model
    from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Bidirectional
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
    from tensorflow.keras.optimizers import Adam
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    warnings.warn("TensorFlow not available. Deep learning models will not be available.")

# Try importing PyTorch as an alternative deep learning framework
try:
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, TensorDataset
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False
    warnings.warn("PyTorch not available. Using TensorFlow or fallback models.")

# Try importing plotly for interactive visualizations
try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False
    warnings.warn("Plotly not available. Using matplotlib for visualizations.")


class PredictiveAnalyticsTools:
    """Tools for advanced predictive analytics and forecasting"""
    
    @staticmethod
    def check_stationarity(time_series: pd.Series) -> Dict:
        """
        Check if a time series is stationary using the Augmented Dickey-Fuller test
        
        Args:
            time_series: Pandas Series with time series data
            
        Returns:
            Dictionary with test results
        """
        result = adfuller(time_series.dropna())
        
        return {
            "test_statistic": result[0],
            "p_value": result[1],
            "is_stationary": result[1] < 0.05,
            "critical_values": result[4],
            "interpretation": "Stationary" if result[1] < 0.05 else "Non-stationary"
        }
    
    @staticmethod
    def forecast_with_prophet(historical_data: pd.DataFrame, 
                             periods: int = 30, 
                             changepoint_prior_scale: float = 0.05,
                             yearly_seasonality: bool = True,
                             weekly_seasonality: bool = True,
                             daily_seasonality: bool = False,
                             use_fallback: bool = True) -> Dict:
        """
        Forecast future values using Facebook Prophet with ARIMA fallback
        
        Args:
            historical_data: DataFrame with historical price data (yfinance format or other)
            periods: Number of periods to forecast
            changepoint_prior_scale: Controls flexibility of trend
            yearly_seasonality: Whether to include yearly seasonality
            weekly_seasonality: Whether to include weekly seasonality
            daily_seasonality: Whether to include daily seasonality
            use_fallback: Whether to use ARIMA as fallback if Prophet fails
            
        Returns:
            Dictionary with forecast results and model components
        """
        # Check if we have valid data
        if historical_data is None or historical_data.empty:
            return {"error": "No historical data provided"}
        
        # Prepare data for forecasting
        try:
            # Handle yfinance DataFrame format
            df = pd.DataFrame()
            
            # If index is DatetimeIndex, use it as ds
            if isinstance(historical_data.index, pd.DatetimeIndex):
                df['ds'] = historical_data.index
                if 'Close' in historical_data.columns:
                    df['y'] = historical_data['Close']
                elif len(historical_data.columns) > 0:
                    # Use the first numeric column if Close isn't available
                    for col in historical_data.columns:
                        if pd.api.types.is_numeric_dtype(historical_data[col]):
                            df['y'] = historical_data[col]
                            break
            # Handle standard format with explicit date column
            elif 'ds' in historical_data.columns and 'y' in historical_data.columns:
                df = historical_data.copy()
            elif 'Date' in historical_data.columns and 'Close' in historical_data.columns:
                df = historical_data.rename(columns={'Date': 'ds', 'Close': 'y'})
            else:
                return {"error": "Could not identify date and value columns in the data"}
            
            # Ensure datetime format is correct and handle timezone issues
            if isinstance(df['ds'], pd.Series):
                df['ds'] = pd.to_datetime(df['ds'])
                # Remove timezone info if present to avoid Prophet errors
                if df['ds'].dt.tz is not None:
                    df['ds'] = df['ds'].dt.tz_localize(None)
        except Exception as e:
            return {"error": f"Error preparing data: {str(e)}"}
        
        # Try Prophet forecasting
        if PROPHET_AVAILABLE:
            try:
                # Create and fit the model
                model = Prophet(
                    changepoint_prior_scale=changepoint_prior_scale,
                    yearly_seasonality=yearly_seasonality,
                    weekly_seasonality=weekly_seasonality,
                    daily_seasonality=daily_seasonality
                )
                model.fit(df)
                
                # Make future dataframe and predict
                future = model.make_future_dataframe(periods=periods)
                forecast = model.predict(future)
                
                # Calculate trend metrics
                trend_change = (forecast['trend'].iloc[-1] / forecast['trend'].iloc[0] - 1) * 100
                trend_direction = "Upward" if trend_change > 0 else "Downward"
                
                # Generate plots
                if PLOTLY_AVAILABLE:
                    # Create interactive Plotly forecast plot
                    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, 
                                        subplot_titles=['Forecast', 'Components'],
                                        vertical_spacing=0.1, row_heights=[0.7, 0.3])
                    
                    # Add historical data
                    fig.add_trace(
                        go.Scatter(x=df['ds'], y=df['y'], mode='markers', name='Historical', 
                                  marker=dict(color='black', size=4)),
                        row=1, col=1
                    )
                    
                    # Add forecast line
                    fig.add_trace(
                        go.Scatter(x=forecast['ds'], y=forecast['yhat'], mode='lines', 
                                  name='Forecast', line=dict(color='blue')),
                        row=1, col=1
                    )
                    
                    # Add uncertainty intervals
                    fig.add_trace(
                        go.Scatter(x=forecast['ds'], y=forecast['yhat_upper'], mode='lines', 
                                  line=dict(width=0), showlegend=False),
                        row=1, col=1
                    )
                    fig.add_trace(
                        go.Scatter(x=forecast['ds'], y=forecast['yhat_lower'], mode='lines',
                                  line=dict(width=0), fill='tonexty', 
                                  fillcolor='rgba(0, 0, 255, 0.2)', name='95% Confidence'),
                        row=1, col=1
                    )
                    
                    # Add trend component
                    fig.add_trace(
                        go.Scatter(x=forecast['ds'], y=forecast['trend'], mode='lines',
                                  name='Trend', line=dict(color='red')),
                        row=2, col=1
                    )
                    
                    fig.update_layout(title='Prophet Forecast', height=800, width=1000)
                    
                    # Save as interactive HTML
                    forecast_plot_path = '/tmp/prophet_forecast.html'
                    with open(forecast_plot_path, 'w') as f:
                        f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))
                    
                    # Also save as PNG for compatibility
                    forecast_png_path = '/tmp/prophet_forecast.png'
                    fig.write_image(forecast_png_path)
                    
                    # Create components plot using Prophet's built-in function
                    fig2 = model.plot_components(forecast)
                    components_plot_path = '/tmp/prophet_components.png'
                    fig2.savefig(components_plot_path)
                    plt.close(fig2)
                    
                else:
                    # Fallback to matplotlib
                    fig1 = model.plot(forecast)
                    fig2 = model.plot_components(forecast)
                    
                    forecast_plot_path = '/tmp/prophet_forecast.png'
                    components_plot_path = '/tmp/prophet_components.png'
                    fig1.savefig(forecast_plot_path)
                    fig2.savefig(components_plot_path)
                    plt.close(fig1)
                    plt.close(fig2)
                
                # Extract relevant data for return
                forecast_data = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(periods).to_dict('records')
                
                # Convert datetime to string for JSON serialization
                for record in forecast_data:
                    record['ds'] = record['ds'].strftime('%Y-%m-%d')
                
                return {
                    "model": "Prophet",
                    "forecast_data": forecast_data,
                    "trend_change_percent": float(trend_change),
                    "trend_direction": trend_direction,
                    "forecast_plot_path": forecast_plot_path,
                    "components_plot_path": components_plot_path
                }
                
            except Exception as e:
                if not use_fallback:
                    return {"error": f"Error in Prophet forecasting: {str(e)}"}
                print(f"Prophet failed with error: {str(e)}. Trying ARIMA fallback.")
        elif not use_fallback:
            return {"error": "Prophet package not available"}
        
        # If Prophet failed or is not available, try ARIMA as fallback
        if ARIMA_AVAILABLE:
            try:
                # Prepare data for ARIMA
                ts_data = df.set_index('ds')['y']
                
                # Fit ARIMA model
                # Start with a simple model (1,1,1) for speed
                model = ARIMA(ts_data, order=(1, 1, 1))
                model_fit = model.fit()
                
                # Forecast
                forecast = model_fit.forecast(steps=periods)
                forecast_index = pd.date_range(start=ts_data.index[-1] + pd.Timedelta(days=1), periods=periods)
                forecast_series = pd.Series(forecast, index=forecast_index)
                
                # Calculate confidence intervals
                pred_conf = model_fit.get_forecast(steps=periods).conf_int()
                lower_series = pred_conf.iloc[:, 0]
                upper_series = pred_conf.iloc[:, 1]
                
                # Plot results
                plt.figure(figsize=(12, 6))
                plt.plot(ts_data.index, ts_data, label='Historical Data')
                plt.plot(forecast_series.index, forecast_series, color='red', label='ARIMA Forecast')
                plt.fill_between(forecast_series.index, 
                               lower_series.values, 
                               upper_series.values, 
                               color='pink', alpha=0.3)
                plt.title('ARIMA Forecast (Prophet Fallback)')
                plt.legend()
                
                # Save plot
                forecast_plot_path = '/tmp/arima_forecast.png'
                plt.savefig(forecast_plot_path)
                plt.close()
                
                # Prepare forecast data in the same format as Prophet
                forecast_data = []
                for i, (date, value, lower, upper) in enumerate(zip(forecast_index, 
                                                                 forecast_series, 
                                                                 lower_series, 
                                                                 upper_series)):
                    forecast_data.append({
                        'ds': date.strftime('%Y-%m-%d'),
                        'yhat': float(value),
                        'yhat_lower': float(lower),
                        'yhat_upper': float(upper)
                    })
                
                # Calculate trend direction
                first_value = forecast_series.iloc[0]
                last_value = forecast_series.iloc[-1]
                trend_change = ((last_value / first_value) - 1) * 100
                trend_direction = "Upward" if trend_change > 0 else "Downward"
                
                return {
                    "model": "ARIMA (Prophet fallback)",
                    "forecast_data": forecast_data,
                    "trend_change_percent": float(trend_change),
                    "trend_direction": trend_direction,
                    "forecast_plot_path": forecast_plot_path,
                    "order": "(1,1,1)"  # ARIMA parameters
                }
                
            except Exception as e:
                return {"error": f"Both Prophet and ARIMA forecasting failed: {str(e)}"}
        
        # If both Prophet and ARIMA failed or are not available, use simple moving average
        try:
            # Prepare data
            ts_data = df.set_index('ds')['y']
            
            # Calculate moving average
            ma_window = min(30, len(ts_data) // 3)  # Use 1/3 of data length or 30, whichever is smaller
            ma = ts_data.rolling(window=ma_window).mean()
            
            # Simple forecast using the average of recent values
            last_value = ts_data.iloc[-1]
            recent_growth = ts_data.pct_change().iloc[-ma_window:].mean()
            
            # Generate forecast
            forecast_index = pd.date_range(start=ts_data.index[-1] + pd.Timedelta(days=1), periods=periods)
            forecast_values = []
            current_value = last_value
            
            for _ in range(periods):
                current_value = current_value * (1 + recent_growth)
                forecast_values.append(current_value)
            
            forecast_series = pd.Series(forecast_values, index=forecast_index)
            
            # Simple confidence intervals (Â±10%)
            lower_series = forecast_series * 0.9
            upper_series = forecast_series * 1.1
            
            # Plot results
            plt.figure(figsize=(12, 6))
            plt.plot(ts_data.index[-90:], ts_data.iloc[-90:], label='Historical Data')
            plt.plot(forecast_series.index, forecast_series, color='green', label='Simple Forecast')
            plt.fill_between(forecast_series.index, 
                           lower_series.values, 
                           upper_series.values, 
                           color='lightgreen', alpha=0.3)
            plt.title('Simple Moving Average Forecast (Last Resort Fallback)')
            plt.legend()
            
            # Save plot
            forecast_plot_path = '/tmp/simple_forecast.png'
            plt.savefig(forecast_plot_path)
            plt.close()
            
            # Prepare forecast data
            forecast_data = []
            for i, (date, value, lower, upper) in enumerate(zip(forecast_index, 
                                                             forecast_series, 
                                                             lower_series, 
                                                             upper_series)):
                forecast_data.append({
                    'ds': date.strftime('%Y-%m-%d'),
                    'yhat': float(value),
                    'yhat_lower': float(lower),
                    'yhat_upper': float(upper)
                })
            
            # Calculate trend direction
            trend_change = ((forecast_series.iloc[-1] / forecast_series.iloc[0]) - 1) * 100
            trend_direction = "Upward" if trend_change > 0 else "Downward"
            
            return {
                "model": "Simple Moving Average (last resort fallback)",
                "forecast_data": forecast_data,
                "trend_change_percent": float(trend_change),
                "trend_direction": trend_direction,
                "forecast_plot_path": forecast_plot_path,
                "window_size": ma_window
            }
            
        except Exception as e:
            return {"error": f"All forecasting methods failed: {str(e)}"}
    
    @staticmethod
    def forecast_with_lstm(historical_data: pd.DataFrame, 
                          price_column: str = 'Close',
                          sequence_length: int = 60,
                          forecast_periods: int = 30,
                          train_split: float = 0.8,
                          epochs: int = 50,
                          batch_size: int = 32) -> Dict:
        """
        Forecast future values using LSTM neural network
        
        Args:
            historical_data: DataFrame with price data
            price_column: Column with price data
            sequence_length: Number of previous time steps to use as input features
            forecast_periods: Number of periods to forecast
            train_split: Proportion of data to use for training
            epochs: Number of training epochs
            batch_size: Batch size for training
            
        Returns:
            Dictionary with forecast results
        """
        if not TENSORFLOW_AVAILABLE:
            return {"error": "TensorFlow not available."}
        
        try:
            # Check if we have valid data
            if price_column not in historical_data.columns:
                return {"error": f"Price column '{price_column}' not found in data"}
            
            # Extract price data
            data = historical_data[price_column].values.reshape(-1, 1)
            
            # Normalize data
            scaler = MinMaxScaler(feature_range=(0, 1))
            scaled_data = scaler.fit_transform(data)
            
            # Create sequences for training
            X, y = [], []
            for i in range(sequence_length, len(scaled_data)):
                X.append(scaled_data[i-sequence_length:i, 0])
                y.append(scaled_data[i, 0])
            X, y = np.array(X), np.array(y)
            
            # Reshape X to be [samples, time steps, features]
            X = np.reshape(X, (X.shape[0], X.shape[1], 1))
            
            # Split into train and test sets
            train_size = int(len(X) * train_split)
            X_train, X_test = X[:train_size], X[train_size:]
            y_train, y_test = y[:train_size], y[train_size:]
            
            # Build LSTM model
            model = Sequential()
            model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
            model.add(Dropout(0.2))
            model.add(LSTM(units=50, return_sequences=False))
            model.add(Dropout(0.2))
            model.add(Dense(units=1))
            
            # Compile and train
            model.compile(optimizer='adam', loss='mean_squared_error')
            model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)
            
            # Evaluate on test set
            y_pred = model.predict(X_test)
            rmse = np.sqrt(np.mean(np.square(y_pred - y_test)))
            
            # Forecast future values
            input_data = scaled_data[-sequence_length:].copy()
            future_predictions = []
            
            for i in range(forecast_periods):
                # Reshape for prediction
                x_input = input_data[-sequence_length:].reshape(1, sequence_length, 1)
                
                # Predict next value
                next_val = model.predict(x_input)[0][0]
                future_predictions.append(next_val)
                
                # Update input data for next prediction
                input_data = np.append(input_data, [[next_val]], axis=0)
            
            # Inverse transform to get actual values
            future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))
            
            # Generate dates for forecast
            last_date = historical_data.index[-1] if isinstance(historical_data.index, pd.DatetimeIndex) else datetime.now()
            forecast_dates = pd.date_range(start=last_date, periods=forecast_periods+1)[1:]
            
            # Plot results
            plt.figure(figsize=(12, 6))
            
            # Plot historical data (last 100 points)
            hist_length = min(100, len(data))
            if isinstance(historical_data.index, pd.DatetimeIndex):
                plt.plot(historical_data.index[-hist_length:], data[-hist_length:], label='Historical')
            else:
                plt.plot(range(hist_length), data[-hist_length:], label='Historical')
            
            # Plot forecast
            plt.plot(forecast_dates, future_predictions, label='LSTM Forecast', color='red')
            plt.title('LSTM Forecast')
            plt.xlabel('Date')
            plt.ylabel('Price')
            plt.legend()
            
            # Save plot
            forecast_plot_path = '/tmp/lstm_forecast.png'
            plt.savefig(forecast_plot_path)
            plt.close()
            
            # Prepare forecast data
            forecast_data = [
                {"date": date.strftime('%Y-%m-%d'), "value": float(value[0])}
                for date, value in zip(forecast_dates, future_predictions)
            ]
            
            return {
                "forecast_data": forecast_data,
                "rmse": float(rmse),
                "forecast_plot_path": forecast_plot_path
            }
            
        except Exception as e:
            return {"error": f"Error in LSTM forecasting: {str(e)}"}
    
    @staticmethod
    def detect_anomalies(time_series: pd.Series, 
                        contamination: float = 0.05,
                        methods: List[str] = ['isolation_forest', 'lof', 'zscore'],
                        window_size: int = 20,
                        zscore_threshold: float = 3.0,
                        return_details: bool = False) -> Dict:
        """
        Detect anomalies in time series data using multiple detection methods
        
        Args:
            time_series: Pandas Series with time series data
            contamination: Expected proportion of anomalies for model-based methods
            methods: List of anomaly detection methods to use
                     Options: 'isolation_forest', 'lof', 'dbscan', 'zscore', 'iqr', 'rolling_zscore'
            window_size: Window size for rolling statistics methods
            zscore_threshold: Threshold for Z-score based methods
            return_details: Whether to return detailed model information
            
        Returns:
            Dictionary with anomaly detection results from multiple methods
        """
        try:
            # Check if we have valid data
            if time_series is None or len(time_series) < 10:
                return {"error": "Insufficient data for anomaly detection"}
            
            # Ensure time_series is a pandas Series with a datetime index
            if not isinstance(time_series, pd.Series):
                try:
                    time_series = pd.Series(time_series)
                except:
                    return {"error": "Could not convert input to pandas Series"}
            
            # Create a copy to avoid modifying the original
            time_series = time_series.copy()
            
            # Handle missing values
            time_series = time_series.interpolate(method='linear')
            
            # Prepare data for model-based methods
            data = time_series.values.reshape(-1, 1)
            
            # Dictionary to store results from each method
            all_results = {}
            all_anomalies = {}
            
            # 1. Isolation Forest
            if 'isolation_forest' in methods:
                try:
                    model_if = IsolationForest(contamination=contamination, random_state=42)
                    anomaly_labels = model_if.fit_predict(data)
                    anomalies_if = pd.Series(anomaly_labels == -1, index=time_series.index)
                    all_anomalies['isolation_forest'] = anomalies_if
                    all_results['isolation_forest'] = {
                        "anomaly_count": int(anomalies_if.sum()),
                        "anomaly_percent": float(anomalies_if.mean() * 100),
                        "anomaly_indices": anomalies_if[anomalies_if].index.tolist(),
                        "anomaly_values": time_series[anomalies_if].tolist(),
                        "description": "Detects anomalies by isolating observations"
                    }
                except Exception as e:
                    all_results['isolation_forest'] = {"error": str(e)}
            
            # 2. Local Outlier Factor
            if 'lof' in methods:
                try:
                    model_lof = LocalOutlierFactor(n_neighbors=20, contamination=contamination)
                    anomaly_labels = model_lof.fit_predict(data)
                    anomalies_lof = pd.Series(anomaly_labels == -1, index=time_series.index)
                    all_anomalies['lof'] = anomalies_lof
                    all_results['lof'] = {
                        "anomaly_count": int(anomalies_lof.sum()),
                        "anomaly_percent": float(anomalies_lof.mean() * 100),
                        "anomaly_indices": anomalies_lof[anomalies_lof].index.tolist(),
                        "anomaly_values": time_series[anomalies_lof].tolist(),
                        "description": "Identifies local deviations in density"
                    }
                except Exception as e:
                    all_results['lof'] = {"error": str(e)}
            
            # 3. DBSCAN
            if 'dbscan' in methods:
                try:
                    # Normalize data for DBSCAN
                    scaler = StandardScaler()
                    data_scaled = scaler.fit_transform(data)
                    
                    # Find optimal eps parameter using nearest neighbors
                    nn = NearestNeighbors(n_neighbors=min(10, len(data_scaled)))
                    nn.fit(data_scaled)
                    distances, _ = nn.kneighbors(data_scaled)
                    distances = np.sort(distances[:, -1])
                    
                    # Use knee point as eps if possible
                    try:
                        from kneed import KneeLocator
                        kneedle = KneeLocator(range(len(distances)), distances, curve='convex', direction='increasing')
                        eps = distances[kneedle.knee] if kneedle.knee else np.median(distances)
                    except ImportError:
                        eps = np.median(distances)
                    
                    model_dbscan = DBSCAN(eps=eps, min_samples=5)
                    cluster_labels = model_dbscan.fit_predict(data_scaled)
                    
                    # Points labeled as -1 are anomalies
                    anomalies_dbscan = pd.Series(cluster_labels == -1, index=time_series.index)
                    all_anomalies['dbscan'] = anomalies_dbscan
                    all_results['dbscan'] = {
                        "anomaly_count": int(anomalies_dbscan.sum()),
                        "anomaly_percent": float(anomalies_dbscan.mean() * 100),
                        "anomaly_indices": anomalies_dbscan[anomalies_dbscan].index.tolist(),
                        "anomaly_values": time_series[anomalies_dbscan].tolist(),
                        "description": "Density-based spatial clustering"
                    }
                except Exception as e:
                    all_results['dbscan'] = {"error": str(e)}
            
            # 4. Z-Score
            if 'zscore' in methods:
                try:
                    mean = np.mean(data)
                    std = np.std(data)
                    z_scores = abs((data - mean) / std)
                    anomalies_zscore = pd.Series(z_scores.reshape(-1) > zscore_threshold, index=time_series.index)
                    all_anomalies['zscore'] = anomalies_zscore
                    all_results['zscore'] = {
                        "anomaly_count": int(anomalies_zscore.sum()),
                        "anomaly_percent": float(anomalies_zscore.mean() * 100),
                        "anomaly_indices": anomalies_zscore[anomalies_zscore].index.tolist(),
                        "anomaly_values": time_series[anomalies_zscore].tolist(),
                        "description": f"Points > {zscore_threshold} standard deviations from mean"
                    }
                except Exception as e:
                    all_results['zscore'] = {"error": str(e)}
            
            # 5. IQR (Interquartile Range)
            if 'iqr' in methods:
                try:
                    Q1 = np.percentile(data, 25)
                    Q3 = np.percentile(data, 75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    anomalies_iqr = pd.Series((data < lower_bound) | (data > upper_bound), index=time_series.index)
                    all_anomalies['iqr'] = anomalies_iqr.reshape(-1)
                    all_results['iqr'] = {
                        "anomaly_count": int(anomalies_iqr.sum()),
                        "anomaly_percent": float(anomalies_iqr.mean() * 100),
                        "anomaly_indices": anomalies_iqr[anomalies_iqr.reshape(-1)].index.tolist(),
                        "anomaly_values": time_series[anomalies_iqr.reshape(-1)].tolist(),
                        "description": "Points outside 1.5 * IQR from quartiles"
                    }
                except Exception as e:
                    all_results['iqr'] = {"error": str(e)}
            
            # 6. Rolling Z-Score
            if 'rolling_zscore' in methods and len(time_series) > window_size:
                try:
                    rolling_mean = time_series.rolling(window=window_size).mean()
                    rolling_std = time_series.rolling(window=window_size).std()
                    rolling_z = (time_series - rolling_mean) / rolling_std
                    anomalies_rolling = pd.Series(abs(rolling_z) > zscore_threshold, index=time_series.index)
                    # Fill NaN values at the beginning
                    anomalies_rolling.iloc[:window_size] = False
                    all_anomalies['rolling_zscore'] = anomalies_rolling
                    all_results['rolling_zscore'] = {
                        "anomaly_count": int(anomalies_rolling.sum()),
                        "anomaly_percent": float(anomalies_rolling.mean() * 100),
                        "anomaly_indices": anomalies_rolling[anomalies_rolling].index.tolist(),
                        "anomaly_values": time_series[anomalies_rolling].tolist(),
                        "description": f"Points > {zscore_threshold} standard deviations from local mean"
                    }
                except Exception as e:
                    all_results['rolling_zscore'] = {"error": str(e)}
            
            # Create ensemble anomaly detection (voting)
            if len(all_anomalies) > 1:
                # Convert all anomaly Series to a DataFrame
                anomaly_df = pd.DataFrame(all_anomalies)
                
                # Count votes for each point
                vote_counts = anomaly_df.sum(axis=1)
                
                # Points flagged by at least 2 methods or 30% of methods (whichever is greater)
                min_votes = max(2, np.ceil(len(methods) * 0.3))
                ensemble_anomalies = vote_counts >= min_votes
                
                all_results['ensemble'] = {
                    "anomaly_count": int(ensemble_anomalies.sum()),
                    "anomaly_percent": float(ensemble_anomalies.mean() * 100),
                    "anomaly_indices": ensemble_anomalies[ensemble_anomalies].index.tolist(),
                    "anomaly_values": time_series[ensemble_anomalies].tolist(),
                    "description": f"Points flagged by at least {min_votes} methods"
                }
            
            # Create visualizations
            try:
                # Use Plotly for interactive visualization if available
                if PLOTLY_AVAILABLE:
                    # Create interactive plot with Plotly
                    fig = go.Figure()
                    
                    # Plot original time series
                    fig.add_trace(go.Scatter(
                        x=time_series.index,
                        y=time_series.values,
                        mode='lines',
                        name='Original Data',
                        line=dict(color='blue', width=2)
                    ))
                    
                    # Plot anomalies from each method with different colors
                    colors = ['red', 'green', 'purple', 'orange', 'brown', 'pink']
                    for i, (method, anomalies) in enumerate(all_anomalies.items()):
                        if anomalies.sum() > 0:  # Only plot if there are anomalies
                            fig.add_trace(go.Scatter(
                                x=time_series[anomalies].index,
                                y=time_series[anomalies].values,
                                mode='markers',
                                name=f'{method.replace("_", " ").title()} Anomalies',
                                marker=dict(color=colors[i % len(colors)], size=10, symbol='x')
                            ))
                    
                    # Plot ensemble anomalies if available
                    if 'ensemble' in all_results and all_results['ensemble']['anomaly_count'] > 0:
                        ensemble_indices = all_results['ensemble']['anomaly_indices']
                        ensemble_values = all_results['ensemble']['anomaly_values']
                        fig.add_trace(go.Scatter(
                            x=ensemble_indices,
                            y=ensemble_values,
                            mode='markers',
                            name='Ensemble Anomalies',
                            marker=dict(color='black', size=12, symbol='circle-open')
                        ))
                    
                    # Add layout details
                    fig.update_layout(
                        title='Multi-Method Anomaly Detection',
                        xaxis_title='Date',
                        yaxis_title='Value',
                        hovermode='closest',
                        legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01),
                        height=600,
                        width=1000
                    )
                    
                    # Save as interactive HTML
                    anomaly_plot_path = '/tmp/anomaly_detection.html'
                    with open(anomaly_plot_path, 'w') as f:
                        f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))
                    
                    # Also save as PNG for compatibility
                    anomaly_png_path = '/tmp/anomaly_detection.png'
                    fig.write_image(anomaly_png_path)
                    
                    # Create comparison table
                    method_stats = pd.DataFrame({
                        'Method': [m.replace('_', ' ').title() for m in all_results.keys()],
                        'Anomaly Count': [all_results[m].get('anomaly_count', 0) for m in all_results.keys()],
                        'Anomaly %': [all_results[m].get('anomaly_percent', 0) for m in all_results.keys()],
                        'Description': [all_results[m].get('description', '') for m in all_results.keys()]
                    })
                    
                    fig_table = go.Figure(data=[go.Table(
                        header=dict(values=list(method_stats.columns),
                                   fill_color='paleturquoise',
                                   align='left'),
                        cells=dict(values=[method_stats[col] for col in method_stats.columns],
                                  fill_color='lavender',
                                  align='left')
                    )])
                    
                    fig_table.update_layout(title='Anomaly Detection Method Comparison')
                    
                    # Save table as HTML
                    anomaly_table_path = '/tmp/anomaly_table.html'
                    with open(anomaly_table_path, 'w') as f:
                        f.write(fig_table.to_html(full_html=False, include_plotlyjs='cdn'))
                    
                else:
                    # Fallback to matplotlib
                    plt.figure(figsize=(12, 6))
                    
                    # Plot original time series
                    plt.plot(time_series.index, time_series.values, label='Original Data')
                    
                    # Plot anomalies from each method with different markers
                    markers = ['x', '+', '^', 's', 'o', 'd']
                    colors = ['red', 'green', 'purple', 'orange', 'brown', 'pink']
                    
                    for i, (method, anomalies) in enumerate(all_anomalies.items()):
                        if anomalies.sum() > 0:  # Only plot if there are anomalies
                            plt.scatter(time_series[anomalies].index, time_series[anomalies].values, 
                                      marker=markers[i % len(markers)], color=colors[i % len(colors)], 
                                      label=f'{method.replace("_", " ").title()} Anomalies')
                    
                    # Plot ensemble anomalies if available
                    if 'ensemble' in all_results and all_results['ensemble']['anomaly_count'] > 0:
                        ensemble_indices = all_results['ensemble']['anomaly_indices']
                        ensemble_values = all_results['ensemble']['anomaly_values']
                        plt.scatter(ensemble_indices, ensemble_values, 
                                  marker='o', edgecolors='black', facecolors='none', s=100, 
                                  label='Ensemble Anomalies')
                    
                    plt.title('Multi-Method Anomaly Detection')
                    plt.xlabel('Date')
                    plt.ylabel('Value')
                    plt.legend()
                    
                    # Save plot
                    anomaly_plot_path = '/tmp/anomaly_detection.png'
                    plt.savefig(anomaly_plot_path)
                    plt.close()
                    
                    # Create comparison table
                    fig, ax = plt.subplots(figsize=(10, len(all_results) * 0.5 + 1))
                    ax.axis('tight')
                    ax.axis('off')
                    
                    table_data = []
                    for method in all_results.keys():
                        if 'error' not in all_results[method]:
                            table_data.append([
                                method.replace('_', ' ').title(),
                                all_results[method].get('anomaly_count', 0),
                                f"{all_results[method].get('anomaly_percent', 0):.2f}%",
                                all_results[method].get('description', '')
                            ])
                    
                    table = ax.table(cellText=table_data,
                                   colLabels=['Method', 'Anomaly Count', 'Anomaly %', 'Description'],
                                   loc='center')
                    
                    table.auto_set_font_size(False)
                    table.set_fontsize(10)
                    table.scale(1.2, 1.2)
                    
                    plt.title('Anomaly Detection Method Comparison')
                    
                    # Save table
                    anomaly_table_path = '/tmp/anomaly_table.png'
                    plt.savefig(anomaly_table_path)
                    plt.close()
            except Exception as e:
                print(f"Warning: Error in anomaly visualization: {str(e)}")
                # Fallback to simple plot
                plt.figure(figsize=(12, 6))
                plt.plot(time_series, label='Original Data')
                
                # Use the first available anomaly detection method
                if len(all_anomalies) > 0:
                    method = list(all_anomalies.keys())[0]
                    anomalies = all_anomalies[method]
                    plt.scatter(time_series[anomalies].index, time_series[anomalies].values, 
                              color='red', label=f'{method.title()} Anomalies')
                
                plt.title('Anomaly Detection')
                plt.xlabel('Date')
                plt.ylabel('Value')
                plt.legend()
                
                # Save plot
                anomaly_plot_path = '/tmp/anomaly_detection.png'
                plt.savefig(anomaly_plot_path)
                plt.close()
            
            # Prepare final results
            final_results = {
                "methods_used": methods,
                "method_results": all_results,
                "ensemble_results": all_results.get('ensemble', {}),
                "anomaly_plot_path": anomaly_plot_path,
                "anomaly_table_path": locals().get('anomaly_table_path', None)
            }
            
            # Add detailed model information if requested
            if return_details:
                final_results["all_anomalies"] = {method: anomalies.tolist() for method, anomalies in all_anomalies.items()}
            
            return final_results
            
        except Exception as e:
            return {"error": f"Error in anomaly detection: {str(e)}"}
    
    @staticmethod
    def calculate_volatility(historical_data: pd.DataFrame, 
                            price_column: str = 'Close',
                            window_size: int = 20,
                            forecast_periods: int = 30,
                            use_garch: bool = True,
                            confidence_level: float = 0.95) -> Dict:
        """
        Calculate historical volatility and forecast future volatility using advanced models
        
        Args:
            historical_data: DataFrame with price data
            price_column: Column with price data
            window_size: Window size for rolling volatility calculation
            forecast_periods: Number of periods to forecast volatility
            use_garch: Whether to use GARCH models for volatility forecasting
            confidence_level: Confidence level for VaR calculation
            
        Returns:
            Dictionary with volatility analysis results and forecasts
        """
        try:
            # Handle multi-index columns if present
            if isinstance(historical_data.columns, pd.MultiIndex):
                if isinstance(price_column, tuple):
                    prices = historical_data[price_column]
                else:
                    # Try to find the price column in multi-index
                    matching_cols = [col for col in historical_data.columns if col[0] == price_column]
                    if matching_cols:
                        prices = historical_data[matching_cols[0]]
                    # Create a synthetic date index
                    historical_data = historical_data.copy().reset_index(drop=True)
                    end_date = datetime.now()
                    start_date = end_date - timedelta(days=len(historical_data))
                    historical_data.index = pd.date_range(start=start_date, periods=len(historical_data))
            
            # Find the appropriate price column
            if price_column not in historical_data.columns:
                # Try to find an appropriate price column
                price_columns = [col for col in historical_data.columns 
                                if col.lower() in ['close', 'price', 'value', 'adjclose']]
                
                if price_columns:
                    price_column = price_columns[0]
                else:
                    # If no suitable column found, use the first numeric column
                    numeric_cols = [col for col in historical_data.columns 
                                  if pd.api.types.is_numeric_dtype(historical_data[col])]
                    if numeric_cols:
                        price_column = numeric_cols[0]
                    else:
                        return {"error": "No suitable price column found in data"}
            
            # Calculate daily returns
            prices = historical_data[price_column]
            returns = prices.pct_change().dropna()
            
            # Calculate multiple volatility measures
            # 1. Simple rolling volatility (standard deviation of returns)
            rolling_vol = returns.rolling(window=window_size).std() * np.sqrt(252)  # Annualized
            
            # 2. Exponentially weighted moving average (EWMA) volatility
            ewma_vol = returns.ewm(span=window_size).std() * np.sqrt(252)  # Annualized
            
            # 3. Parkinson volatility (uses high-low range)
            parkinson_vol = None
            if 'High' in historical_data.columns and 'Low' in historical_data.columns:
                high = historical_data['High']
                low = historical_data['Low']
                # Parkinson volatility formula
                log_hl = np.log(high / low)
                parkinson_vol = np.sqrt(1 / (4 * np.log(2)) * log_hl**2).rolling(window=window_size).mean() * np.sqrt(252)
            
            # Calculate historical volatility statistics
            current_volatility = rolling_vol.iloc[-1]
            avg_volatility = rolling_vol.mean()
            max_volatility = rolling_vol.max()
            min_volatility = rolling_vol.min()
            
            # Calculate volatility of volatility
            vol_of_vol = rolling_vol.pct_change().dropna().std() * np.sqrt(252)
            
            # Determine volatility regime
            if current_volatility < avg_volatility * 0.8:
                volatility_regime = "Low"
            elif current_volatility > avg_volatility * 1.2:
                volatility_regime = "High"
            else:
                volatility_regime = "Normal"
            
            # Calculate volatility trend
            recent_trend = rolling_vol.iloc[-10:].mean() - rolling_vol.iloc[-30:-10].mean() if len(rolling_vol) >= 30 else 0
            volatility_trend = "Increasing" if recent_trend > 0 else "Decreasing"
            
            # Calculate Value at Risk (VaR)
            var_historical = np.percentile(returns, (1 - confidence_level) * 100) * prices.iloc[-1]
            # Use scipy.stats for parametric VaR
            from scipy import stats
            var_parametric = stats.norm.ppf(1 - confidence_level) * returns.std() * prices.iloc[-1]
            
            # Forecast volatility
            forecast_vol = None
            forecast_dates = None
            garch_forecast = None
            garch_model_summary = None
            garch_plot_path = None
            
            try:
                # Try to use GARCH model if requested
                if use_garch:
                    # Check if arch module is available
                    try:
                        from arch import arch_model
                        
                        # Fit GARCH model
                        garch_model = arch_model(returns.values * 100, vol='Garch', p=1, q=1, mean='Zero')
                        garch_result = garch_model.fit(disp='off')
                        garch_model_summary = str(garch_result.summary())
                        
                        # Forecast volatility
                        garch_forecast = garch_result.forecast(horizon=forecast_periods)
                        forecast_vol = np.sqrt(garch_forecast.variance.values[-1, :]) / 100 * np.sqrt(252)  # Annualized
                        
                        # Generate forecast dates
                        last_date = historical_data.index[-1]
                        forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_periods)
                        
                        # Plot GARCH forecast
                        plt.figure(figsize=(12, 6))
                        plt.plot(garch_result.conditional_volatility * np.sqrt(252), label='GARCH Volatility')
                        plt.title('GARCH Volatility Model')
                        plt.xlabel('Date')
                        plt.ylabel('Annualized Volatility')
                        plt.legend()
                        
                        # Save GARCH plot
                        garch_plot_path = '/tmp/garch_volatility.png'
                        plt.savefig(garch_plot_path)
                        plt.close()
                        
                    except ImportError:
                        # Fallback to simple forecasting if arch module is not available
                        forecast_vol = np.array([current_volatility] * forecast_periods)
                        last_date = historical_data.index[-1]
                        forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_periods)
                else:
                    # Simple volatility forecast using EWMA
                    forecast_vol = np.array([ewma_vol.iloc[-1]] * forecast_periods)
                    last_date = historical_data.index[-1]
                    forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_periods)
            except Exception as e:
                # Fallback to simple forecasting if GARCH fails
                forecast_vol = np.array([current_volatility] * forecast_periods)
                last_date = historical_data.index[-1]
                forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_periods)
                print(f"Warning: GARCH modeling failed with error: {str(e)}. Using simple forecast instead.")
            
            # Create visualizations
            try:
                # Use Plotly for interactive visualization if available
                if PLOTLY_AVAILABLE:
                    # Create interactive plot with Plotly
                    fig = go.Figure()
                    
                    # Plot historical volatility
                    fig.add_trace(go.Scatter(
                        x=rolling_vol.index,
                        y=rolling_vol,
                        mode='lines',
                        name='Rolling Volatility',
                        line=dict(color='blue', width=2)
                    ))
                    
                    # Plot EWMA volatility
                    fig.add_trace(go.Scatter(
                        x=ewma_vol.index,
                        y=ewma_vol,
                        mode='lines',
                        name='EWMA Volatility',
                        line=dict(color='green', width=2)
                    ))
                    
                    # Plot Parkinson volatility if available
                    if parkinson_vol is not None:
                        fig.add_trace(go.Scatter(
                            x=parkinson_vol.index,
                            y=parkinson_vol,
                            mode='lines',
                            name='Parkinson Volatility',
                            line=dict(color='purple', width=2)
                        ))
                    
                    # Plot average volatility line
                    fig.add_trace(go.Scatter(
                        x=[rolling_vol.index[0], rolling_vol.index[-1]],
                        y=[avg_volatility, avg_volatility],
                        mode='lines',
                        name='Average Volatility',
                        line=dict(color='red', width=2, dash='dash')
                    ))
                    
                    # Plot volatility forecast
                    if forecast_vol is not None and forecast_dates is not None:
                        fig.add_trace(go.Scatter(
                            x=forecast_dates,
                            y=forecast_vol,
                            mode='lines',
                            name='Volatility Forecast',
                            line=dict(color='orange', width=2)
                        ))
                    
                    # Add layout details
                    fig.update_layout(
                        title='Volatility Analysis and Forecast',
                        xaxis_title='Date',
                        yaxis_title='Annualized Volatility',
                        hovermode='x unified',
                        legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01),
                        height=600,
                        width=1000
                    )
                    
                    # Save as interactive HTML
                    volatility_plot_path = '/tmp/volatility_analysis.html'
                    with open(volatility_plot_path, 'w') as f:
                        f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))
                    
                    # Also save as PNG for compatibility
                    volatility_png_path = '/tmp/volatility_analysis.png'
                    fig.write_image(volatility_png_path)
                    
                    # Create volatility regime distribution chart
                    vol_regimes = pd.cut(rolling_vol, bins=[0, avg_volatility*0.8, avg_volatility*1.2, float('inf')],
                                         labels=['Low', 'Normal', 'High'])
                    regime_counts = vol_regimes.value_counts()
                    
                    fig_pie = go.Figure(data=[go.Pie(
                        labels=regime_counts.index,
                        values=regime_counts.values,
                        hole=.3,
                        marker_colors=['green', 'blue', 'red']
                    )])
                    
                    fig_pie.update_layout(title='Volatility Regime Distribution')
                    
                    # Save pie chart
                    volatility_regime_path = '/tmp/volatility_regimes.html'
                    with open(volatility_regime_path, 'w') as f:
                        f.write(fig_pie.to_html(full_html=False, include_plotlyjs='cdn'))
                    
                else:
                    # Fallback to matplotlib
                    plt.figure(figsize=(12, 6))
                    
                    # Plot historical volatility
                    plt.plot(rolling_vol.index, rolling_vol, label='Rolling Volatility', color='blue')
                    plt.plot(ewma_vol.index, ewma_vol, label='EWMA Volatility', color='green')
                    
                    # Plot Parkinson volatility if available
                    if parkinson_vol is not None:
                        plt.plot(parkinson_vol.index, parkinson_vol, label='Parkinson Volatility', color='purple')
                    
                    # Plot average volatility line
                    plt.axhline(y=avg_volatility, color='r', linestyle='--', label='Average Volatility')
                    
                    # Plot volatility forecast
                    if forecast_vol is not None and forecast_dates is not None:
                        plt.plot(forecast_dates, forecast_vol, label='Volatility Forecast', color='orange')
                    
                    plt.title('Volatility Analysis and Forecast')
                    plt.xlabel('Date')
                    plt.ylabel('Annualized Volatility')
                    plt.legend()
                    
                    # Save plot
                    volatility_plot_path = '/tmp/volatility_analysis.png'
                    plt.savefig(volatility_plot_path)
                    plt.close()
                    
                    # Create volatility regime distribution chart
                    vol_regimes = pd.cut(rolling_vol, bins=[0, avg_volatility*0.8, avg_volatility*1.2, float('inf')],
                                         labels=['Low', 'Normal', 'High'])
                    regime_counts = vol_regimes.value_counts()
                    
                    plt.figure(figsize=(8, 8))
                    plt.pie(regime_counts.values, labels=regime_counts.index, autopct='%1.1f%%',
                           colors=['green', 'blue', 'red'])
                    plt.title('Volatility Regime Distribution')
                    
                    # Save pie chart
                    volatility_regime_path = '/tmp/volatility_regimes.png'
                    plt.savefig(volatility_regime_path)
                    plt.close()
            except Exception as e:
                print(f"Warning: Error in volatility visualization: {str(e)}")
                # Fallback to simple plot
                plt.figure(figsize=(12, 6))
                plt.plot(rolling_vol.index, rolling_vol, label='Rolling Volatility')
                plt.axhline(y=avg_volatility, color='r', linestyle='-', label='Average Volatility')
                plt.title('Historical Volatility (Annualized)')
                plt.xlabel('Date')
                plt.ylabel('Volatility')
                plt.legend()
                
                # Save plot
                volatility_plot_path = '/tmp/volatility_analysis.png'
                plt.savefig(volatility_plot_path)
                plt.close()
            
            # Prepare forecast data
            forecast_data = None
            if forecast_vol is not None and forecast_dates is not None:
                forecast_data = [
                    {"date": date.strftime('%Y-%m-%d'), "volatility": float(vol)}
                    for date, vol in zip(forecast_dates, forecast_vol)
                ]
            
            # Prepare comprehensive results
            results = {
                "current_volatility": float(current_volatility),
                "average_volatility": float(avg_volatility),
                "max_volatility": float(max_volatility),
                "min_volatility": float(min_volatility),
                "volatility_of_volatility": float(vol_of_vol),
                "volatility_regime": volatility_regime,
                "volatility_trend": volatility_trend,
                "volatility_change_percent": float(((current_volatility / avg_volatility) - 1) * 100),
                "value_at_risk": {
                    "historical": float(var_historical),
                    "parametric": float(var_parametric),
                    "confidence_level": confidence_level
                },
                "volatility_plot_path": volatility_plot_path,
                "volatility_regime_path": locals().get('volatility_regime_path', None)
            }
            
            # Add GARCH results if available
            if garch_forecast is not None:
                results["garch_model"] = {
                    "available": True,
                    "plot_path": garch_plot_path,
                    "forecast": forecast_data
                }
            
            # Add Parkinson volatility if available
            if parkinson_vol is not None:
                results["parkinson_volatility"] = {
                    "current": float(parkinson_vol.iloc[-1]),
                    "average": float(parkinson_vol.mean())
                }
            
            return results
        except Exception as e:
            return {"error": f"Error in scenario analysis: {str(e)}"}
        
        # Prepare comprehensive results
        results = {
            "scenario_results": scenario_results,
            "scenario_plot_path": scenario_plot_path,
            "scenario_table_path": scenario_table_path
        }
        
        return results
                                if col.lower() in ['close', 'price', 'value', 'adjclose']]
                
                if price_columns:
                    price_column = price_columns[0]
                else:
                    # If no suitable column found, use the first numeric column
                    numeric_cols = [col for col in historical_data.columns 
                                  if pd.api.types.is_numeric_dtype(historical_data[col])]
                    if numeric_cols:
                        price_column = numeric_cols[0]
                    else:
                        return {"error": "No suitable price column found in data"}
            
            # Calculate historical returns and volatility
            prices = historical_data[price_column]
            returns = prices.pct_change().dropna()
            
            # Calculate key statistics
            mean_return = returns.mean()
            volatility = returns.std()
            last_price = prices.iloc[-1]
            
            # Define default scenario parameters
            default_scenarios = {
                'base': {'daily_return': mean_return, 'vol_multiplier': 1.0, 'description': 'Current trend continues'},
                'bull': {'daily_return': max(mean_return * 2, mean_return + 0.001), 'vol_multiplier': 0.8, 'description': 'Optimistic outlook with higher returns and lower volatility'},
                'bear': {'daily_return': min(mean_return * -1, -0.0005), 'vol_multiplier': 1.5, 'description': 'Pessimistic outlook with negative returns and higher volatility'}
            }
            
            # Use custom scenarios if provided
            scenario_params = custom_scenarios if custom_scenarios else default_scenarios
            
            # Generate scenarios
            scenario_results = {}
            all_simulations = {}  # Store all simulation paths for interactive plotting
            
            for scenario in scenarios:
                if scenario not in scenario_params:
                    continue
                
                # Get scenario parameters
                params = scenario_params[scenario]
                daily_return = params['daily_return']
                vol = volatility * params['vol_multiplier']
                
                # Simulate future prices using Monte Carlo
                simulation_df = pd.DataFrame()
                all_paths = []  # Store all paths for this scenario
                
                for i in range(simulations):
                    prices = [last_price]
                    
                    for j in range(forecast_periods):
                        # Generate random return from normal distribution
                        random_return = np.random.normal(daily_return, vol)
                        next_price = prices[-1] * (1 + random_return)
                        prices.append(next_price)
                    
                    simulation_df[i] = prices
                    all_paths.append(prices[1:])  # Skip the first element (last historical price)
                
                all_simulations[scenario] = all_paths
                
                # Calculate statistics from simulations
                mean_path = simulation_df.mean(axis=1)
                median_path = simulation_df.median(axis=1)
                upper_path = simulation_df.quantile(0.95, axis=1)
                lower_path = simulation_df.quantile(0.05, axis=1)
                
                # Generate dates for forecast
                try:
                    last_date = historical_data.index[-1]
                    forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_periods)
                    
                    # Make sure we have the right number of elements in each path
                    # The simulation paths should start from index 1 (skipping the initial price)
                    path_length = len(forecast_dates)
                    
                    # Store results
                    scenario_results[scenario] = {
                        'dates': forecast_dates.strftime('%Y-%m-%d').tolist(),
                        'mean_path': mean_path[1:path_length+1].tolist(),  # Skip the first element (last historical price)
                        'median_path': median_path[1:path_length+1].tolist(),
                        'upper_path': upper_path[1:path_length+1].tolist(),
                        'lower_path': lower_path[1:path_length+1].tolist(),
                        'final_mean_price': float(mean_path.iloc[-1]),
                        'final_median_price': float(median_path.iloc[-1]),
                        'final_upper_price': float(upper_path.iloc[-1]),
                        'final_lower_price': float(lower_path.iloc[-1]),
                        'expected_return': float(((mean_path.iloc[-1] / last_price) - 1) * 100),
                        'worst_case_return': float(((lower_path.iloc[-1] / last_price) - 1) * 100),
                        'best_case_return': float(((upper_path.iloc[-1] / last_price) - 1) * 100),
                        'description': params.get('description', '')
                    }
                except Exception as e:
                    return {"error": f"Error in scenario analysis date handling: {str(e)}"}
            
            # Create visualizations
            try:
                # Use Plotly for interactive visualization if available
                if PLOTLY_AVAILABLE:
                    # Create interactive plot with Plotly
                    fig = go.Figure()
                    
                    # Plot historical data - use last 60 days or all if less
                    history_length = min(60, len(historical_data))
                    historical_dates = historical_data.index[-history_length:]
                    historical_prices = historical_data[price_column].iloc[-history_length:]
                    
                    fig.add_trace(go.Scatter(
                        x=historical_dates,
                        y=historical_prices,
                        mode='lines',
                        name='Historical',
                        line=dict(color='black', width=2)
                    ))
                    
                    # Plot each scenario
                    colors = {'base': 'blue', 'bull': 'green', 'bear': 'red'}
                    
                    for scenario, results in scenario_results.items():
                        dates = pd.to_datetime(results['dates'])
                        
                        # Add mean path
                        fig.add_trace(go.Scatter(
                            x=dates,
                            y=results['mean_path'],
                            mode='lines',
                            name=f'{scenario.capitalize()} Case',
                            line=dict(color=colors.get(scenario, 'gray'), width=2)
                        ))
                        
                        # Add confidence interval - ensure lists are the same length
                        try:
                            upper_path = results['upper_path']
                            lower_path = results['lower_path']
                            
                            # Ensure both lists have the same length
                            min_length = min(len(upper_path), len(lower_path))
                            upper_path = upper_path[:min_length]
                            lower_path = lower_path[:min_length]
                            dates_list = list(dates)[:min_length]
                            
                            fig.add_trace(go.Scatter(
                                x=dates_list + dates_list[::-1],
                                y=upper_path + lower_path[::-1],
                                fill='toself',
                                fillcolor=f'rgba({"0,0,255" if scenario == "base" else "0,128,0" if scenario == "bull" else "255,0,0"},0.2)',
                                line=dict(color='rgba(255,255,255,0)'),
                                name=f'{scenario.capitalize()} 90% Confidence',
                                showlegend=False
                            ))
                        except Exception as e:
                            print(f"Warning: Could not add confidence interval for {scenario}: {str(e)}")
                            # Continue without the confidence interval
                    
                    # Add layout details
                    fig.update_layout(
                        title='Monte Carlo Scenario Analysis',
                        xaxis_title='Date',
                        yaxis_title='Price',
                        hovermode='x unified',
                        legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01),
                        height=600,
                        width=1000
                    )
                    
                    # Save as interactive HTML
                    scenario_plot_path = '/tmp/scenario_analysis.html'
                    with open(scenario_plot_path, 'w') as f:
                        f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))
                    
                    # Also save as PNG for compatibility
                    scenario_png_path = '/tmp/scenario_analysis.png'
                    fig.write_image(scenario_png_path)
                    
                    # Create a table with scenario statistics
                    scenario_stats = pd.DataFrame({
                        'Scenario': [s.capitalize() for s in scenario_results.keys()],
                        'Final Price': [scenario_results[s]['final_mean_price'] for s in scenario_results.keys()],
                        'Expected Return (%)': [scenario_results[s]['expected_return'] for s in scenario_results.keys()],
                        'Worst Case (%)': [scenario_results[s]['worst_case_return'] for s in scenario_results.keys()],
                        'Best Case (%)': [scenario_results[s]['best_case_return'] for s in scenario_results.keys()]
                    })
                    
                    # Create a table visualization
                    fig_table = go.Figure(data=[go.Table(
                        header=dict(values=list(scenario_stats.columns),
                                   fill_color='paleturquoise',
                                   align='left'),
                        cells=dict(values=[scenario_stats[col] for col in scenario_stats.columns],
                                  fill_color='lavender',
                                  align='left',
                                  format=[None, '.2f', '.2f%', '.2f%', '.2f%'])
                    )])
                    
                    fig_table.update_layout(title='Scenario Analysis Summary')
                    
                    # Save table as HTML
                    scenario_table_path = '/tmp/scenario_table.html'
                    with open(scenario_table_path, 'w') as f:
                        f.write(fig_table.to_html(full_html=False, include_plotlyjs='cdn'))
                    
                else:
                    # Fallback to matplotlib
                    plt.figure(figsize=(12, 6))
                    
                    # Plot historical data - use last 30 days or all if less
                    history_length = min(30, len(historical_data))
                    historical_dates = historical_data.index[-history_length:]
                    historical_prices = historical_data[price_column].iloc[-history_length:]
                    plt.plot(historical_dates, historical_prices, label='Historical', color='black')
                    
                    # Plot each scenario
                    colors = {'base': 'blue', 'bull': 'green', 'bear': 'red'}
                    
                    for scenario, results in scenario_results.items():
                        try:
                            dates = pd.to_datetime(results['dates'])
                            mean_path = results['mean_path']
                            lower_path = results['lower_path']
                            upper_path = results['upper_path']
                            
                            # Ensure all arrays have the same length
                            min_length = min(len(dates), len(mean_path), len(lower_path), len(upper_path))
                            dates = dates[:min_length]
                            mean_path = mean_path[:min_length]
                            lower_path = lower_path[:min_length]
                            upper_path = upper_path[:min_length]
                            
                            plt.plot(dates, mean_path, label=f'{scenario.capitalize()} Case', color=colors.get(scenario, 'gray'))
                            plt.fill_between(dates, lower_path, upper_path, alpha=0.2, color=colors.get(scenario, 'gray'))
                        except Exception as e:
                            print(f"Warning: Could not plot scenario {scenario}: {str(e)}")
                            # Continue without this scenario
                    
                    plt.title('Monte Carlo Scenario Analysis')
                    plt.xlabel('Date')
                    plt.ylabel('Price')
                    plt.legend()
                    
                    # Save plot
                    scenario_plot_path = '/tmp/scenario_analysis.png'
                    plt.savefig(scenario_plot_path)
                    plt.close()
                    
                    # Create a table with scenario statistics
                    fig, ax = plt.subplots(figsize=(10, 3))
                    ax.axis('tight')
                    ax.axis('off')
                    
                    table_data = []
                    for scenario in scenario_results.keys():
                        s = scenario_results[scenario]
                        table_data.append([scenario.capitalize(), 
                                          f"${s['final_mean_price']:.2f}", 
                                          f"{s['expected_return']:.2f}%", 
                                          f"{s['worst_case_return']:.2f}%", 
                                          f"{s['best_case_return']:.2f}%"])
                    
                    table = ax.table(cellText=table_data,
                                   colLabels=['Scenario', 'Final Price', 'Expected Return', 'Worst Case', 'Best Case'],
                                   loc='center')
                    
                    table.auto_set_font_size(False)
                    table.set_fontsize(10)
                    table.scale(1.2, 1.2)
                    
                    plt.title('Scenario Analysis Summary')
                    
                    # Save table
                    scenario_table_path = '/tmp/scenario_table.png'
                    plt.savefig(scenario_table_path)
                    plt.close()
            except Exception as e:
                return {"error": f"Error in scenario analysis visualization: {str(e)}"}
            
            # Return comprehensive results
            return {
                "scenarios": scenario_results,
                "last_price": float(last_price),
                "forecast_periods": forecast_periods,
                "simulations": simulations,
                "historical_stats": {
                    "mean_return": float(mean_return),
                    "annualized_return": float(mean_return * 252 * 100),  # Annualized percentage
                    "volatility": float(volatility),
                    "annualized_volatility": float(volatility * np.sqrt(252) * 100)  # Annualized percentage
                },
                "scenario_plot_path": scenario_plot_path,
                "scenario_table_path": locals().get('scenario_table_path', None)
            }
            
        except Exception as e:
            return {"error": f"Error in scenario analysis: {str(e)}"}
